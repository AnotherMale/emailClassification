{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12608861,"sourceType":"datasetVersion","datasetId":7964680}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vibhavohri/urgency-keyword-detection-with-bert?scriptVersionId=253099308\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\nfrom transformers import BertTokenizerFast, BertModel\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport itertools\n\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\ndf = pd.read_csv(\"/kaggle/input/urgency-labeled-enron-emails/FinalCleanDatabase.csv\")\ndf = df.dropna(subset=['Urgency'])\nprint(df.Urgency.value_counts())\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', use_fast=True)\nMAX_LEN = 128\n\ndef encode_texts(texts):\n    \"\"\"\n    Encode a list of texts into token IDs and attention masks for BERT\n    \"\"\"\n    with torch.no_grad():\n        return tokenizer(\n            list(texts),\n            padding='max_length',\n            truncation=True,\n            max_length=MAX_LEN,\n            return_tensors='pt'\n        )\n\nclass PhishingEmailClassifier(nn.Module):\n    \"\"\"\n    A BERT-based classifier for multi-class urgency detection.\n    \"\"\"\n    def __init__(self, num_classes=6, unfreeze_bert=False):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        if not unfreeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_token = output.last_hidden_state[:, 0, :]\n        return self.classifier(self.dropout(cls_token))\n\ndef train_model(model, train_loader, val_loader, lr=2e-5, epochs=2, patience=2, fold=0, device='cpu'):\n    \"\"\"\n    Train the model with early stopping for multi-class classification.\n    \"\"\"\n    model = model.to(device)\n    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    best_loss = float('inf')\n    patience_counter = 0\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    best_model_path = f\"best_model_fold_{fold}_{timestamp}.pt\"\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n\n        for batch in train_loader:\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            optimizer.zero_grad()\n            logits = model(input_ids, attention_mask)\n            loss = criterion(logits, labels.long())\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        model.eval()\n        total_val_loss = 0\n        all_preds, all_labels = [], []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n                logits = model(input_ids, attention_mask)\n                loss = criterion(logits, labels.long())\n                total_val_loss += loss.item()\n                preds = torch.argmax(logits, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), best_model_path)\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                break\n\n    model.load_state_dict(torch.load(best_model_path))\n    return model\n\ndef evaluate_model(model, val_loader, device='cpu'):\n    \"\"\"\n    Evaluate the model's performance on validation data.\n    \"\"\"\n    model.eval()\n    all_logits, all_labels = [], []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            logits = model(input_ids, attention_mask)\n            all_logits.append(logits)\n            all_labels.append(labels)\n\n    all_logits = torch.cat(all_logits)\n    all_labels = torch.cat(all_labels)\n    preds = torch.argmax(all_logits, dim=1).cpu().numpy()\n    true_labels = all_labels.cpu().numpy()\n\n    return (\n        accuracy_score(true_labels, preds),\n        f1_score(true_labels, preds, average='weighted'),\n        precision_score(true_labels, preds, average='weighted'),\n        recall_score(true_labels, preds, average='weighted'),\n        confusion_matrix(true_labels, preds)\n    )\n\nX = df.Body.values\ny = df.Urgency.values\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n\ndf['Body'] = df['Body'].fillna('')\nencodings = tokenizer(\n    list(df['Body']),\n    padding='max_length',\n    truncation=True,\n    max_length=MAX_LEN,\n    return_tensors='pt'\n)\nall_input_ids = encodings['input_ids']\nall_attention_mask = encodings['attention_mask']\nall_labels = torch.tensor(df['Urgency'].values, dtype=torch.long)\n\nbatch_sizes = [16, 32]\nlearning_rates = [2e-5, 3e-5]\nEPOCHS = 2\nFOLDS = 3\n\nresults = []\n\nfor batch_size, lr in itertools.product(batch_sizes, learning_rates):\n    print(f\"Testing: Batch Size={batch_size}, Learning Rate={lr}\")\n    fold_metrics = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        train_dataset = TensorDataset(\n            all_input_ids[train_idx],\n            all_attention_mask[train_idx],\n            all_labels[train_idx]\n        )\n        val_dataset = TensorDataset(\n            all_input_ids[val_idx],\n            all_attention_mask[val_idx],\n            all_labels[val_idx]\n        )\n\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n        model = PhishingEmailClassifier(num_classes=6, unfreeze_bert=False)\n        trained_model = train_model(model, train_loader, val_loader, lr=lr, epochs=EPOCHS, patience=2, fold=fold, device='cuda' if torch.cuda.is_available() else 'cpu')\n\n        acc, f1, prec, rec, cm = evaluate_model(trained_model, val_loader, device='cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\" Fold {fold} -- Acc: {acc:.4f} | F1: {f1:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f}\")\n        fold_metrics.append((acc, f1, prec, rec))\n\n    avg_metrics = np.mean(fold_metrics, axis=0)\n    results.append({\n        \"batch_size\": batch_size,\n        \"learning_rate\": lr,\n        \"avg_accuracy\": avg_metrics[0],\n        \"avg_f1\": avg_metrics[1],\n        \"avg_precision\": avg_metrics[2],\n        \"avg_recall\": avg_metrics[3],\n    })\n\nresult_df = pd.DataFrame(results)\nprint(result_df.sort_values(by=\"avg_f1\", ascending=False))\n\nplt.figure(figsize=(10,6))\nsns.barplot(data=result_df, x=\"batch_size\", y=\"avg_f1\", hue=\"learning_rate\")\nplt.title(\"F1 Score across Batch Size and Learning Rate\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}